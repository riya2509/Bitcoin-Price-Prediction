{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01b9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a33391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>priceUSD</th>\n",
       "      <th>transactions</th>\n",
       "      <th>size</th>\n",
       "      <th>sentbyaddress</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>hashrate</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>transactionfeesUSD</th>\n",
       "      <th>...</th>\n",
       "      <th>price3rsiUSD</th>\n",
       "      <th>price7rsiUSD</th>\n",
       "      <th>price14rsiUSD</th>\n",
       "      <th>price30rsiUSD</th>\n",
       "      <th>price90rsiUSD</th>\n",
       "      <th>price3rocUSD</th>\n",
       "      <th>price7rocUSD</th>\n",
       "      <th>price14rocUSD</th>\n",
       "      <th>price30rocUSD</th>\n",
       "      <th>price90rocUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010/07/17</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>235</td>\n",
       "      <td>649.653</td>\n",
       "      <td>390</td>\n",
       "      <td>181.543</td>\n",
       "      <td>2.775561e+09</td>\n",
       "      <td>154298.0</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010/07/18</td>\n",
       "      <td>0.0726</td>\n",
       "      <td>248</td>\n",
       "      <td>765.285</td>\n",
       "      <td>424</td>\n",
       "      <td>181.543</td>\n",
       "      <td>1.554461e+09</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010/07/19</td>\n",
       "      <td>0.0859</td>\n",
       "      <td>354</td>\n",
       "      <td>756.040</td>\n",
       "      <td>553</td>\n",
       "      <td>181.543</td>\n",
       "      <td>1.551287e+09</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>4048.0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010/07/20</td>\n",
       "      <td>0.0783</td>\n",
       "      <td>413</td>\n",
       "      <td>984.707</td>\n",
       "      <td>632</td>\n",
       "      <td>181.543</td>\n",
       "      <td>1.640430e+09</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>82.751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010/07/21</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>256</td>\n",
       "      <td>542.483</td>\n",
       "      <td>440</td>\n",
       "      <td>181.543</td>\n",
       "      <td>1.723493e+09</td>\n",
       "      <td>460783.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>78.603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 737 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  priceUSD  transactions     size  sentbyaddress  difficulty  \\\n",
       "0  2010/07/17    0.0495           235  649.653            390     181.543   \n",
       "1  2010/07/18    0.0726           248  765.285            424     181.543   \n",
       "2  2010/07/19    0.0859           354  756.040            553     181.543   \n",
       "3  2010/07/20    0.0783           413  984.707            632     181.543   \n",
       "4  2010/07/21    0.0767           256  542.483            440     181.543   \n",
       "\n",
       "       hashrate  mining_profitability  sentinusdUSD  transactionfeesUSD  ...  \\\n",
       "0  2.775561e+09              154298.0        1193.0            0.000010  ...   \n",
       "1  1.554461e+09              401834.0        2620.0            0.000243  ...   \n",
       "2  1.551287e+09              481473.0        4048.0            0.000022  ...   \n",
       "3  1.640430e+09              431831.0        2341.0            0.000000  ...   \n",
       "4  1.723493e+09              460783.0        2122.0            0.000000  ...   \n",
       "\n",
       "   price3rsiUSD  price7rsiUSD  price14rsiUSD  price30rsiUSD  price90rsiUSD  \\\n",
       "0         0.000           0.0            0.0            0.0            0.0   \n",
       "1         0.000           0.0            0.0            0.0            0.0   \n",
       "2         0.000           0.0            0.0            0.0            0.0   \n",
       "3        82.751           0.0            0.0            0.0            0.0   \n",
       "4        78.603           0.0            0.0            0.0            0.0   \n",
       "\n",
       "   price3rocUSD  price7rocUSD  price14rocUSD  price30rocUSD  price90rocUSD  \n",
       "0         0.000           0.0            0.0            0.0            0.0  \n",
       "1         0.000           0.0            0.0            0.0            0.0  \n",
       "2         0.000           0.0            0.0            0.0            0.0  \n",
       "3        58.099           0.0            0.0            0.0            0.0  \n",
       "4         5.652           0.0            0.0            0.0            0.0  \n",
       "\n",
       "[5 rows x 737 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('btc_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42e09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75c4cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceUSD</th>\n",
       "      <th>transactions</th>\n",
       "      <th>size</th>\n",
       "      <th>sentbyaddress</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>hashrate</th>\n",
       "      <th>mining_profitability</th>\n",
       "      <th>sentinusdUSD</th>\n",
       "      <th>transactionfeesUSD</th>\n",
       "      <th>median_transaction_feeUSD</th>\n",
       "      <th>...</th>\n",
       "      <th>price3rsiUSD</th>\n",
       "      <th>price7rsiUSD</th>\n",
       "      <th>price14rsiUSD</th>\n",
       "      <th>price30rsiUSD</th>\n",
       "      <th>price90rsiUSD</th>\n",
       "      <th>price3rocUSD</th>\n",
       "      <th>price7rocUSD</th>\n",
       "      <th>price14rocUSD</th>\n",
       "      <th>price30rocUSD</th>\n",
       "      <th>price90rocUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0495</td>\n",
       "      <td>235</td>\n",
       "      <td>649.653</td>\n",
       "      <td>390</td>\n",
       "      <td>181.543</td>\n",
       "      <td>2.775561e+09</td>\n",
       "      <td>154298.0</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0726</td>\n",
       "      <td>248</td>\n",
       "      <td>765.285</td>\n",
       "      <td>424</td>\n",
       "      <td>181.543</td>\n",
       "      <td>1.554461e+09</td>\n",
       "      <td>401834.0</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0859</td>\n",
       "      <td>354</td>\n",
       "      <td>756.040</td>\n",
       "      <td>553</td>\n",
       "      <td>181.543</td>\n",
       "      <td>1.551287e+09</td>\n",
       "      <td>481473.0</td>\n",
       "      <td>4048.0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0783</td>\n",
       "      <td>413</td>\n",
       "      <td>984.707</td>\n",
       "      <td>632</td>\n",
       "      <td>181.543</td>\n",
       "      <td>1.640430e+09</td>\n",
       "      <td>431831.0</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0767</td>\n",
       "      <td>256</td>\n",
       "      <td>542.483</td>\n",
       "      <td>440</td>\n",
       "      <td>181.543</td>\n",
       "      <td>1.723493e+09</td>\n",
       "      <td>460783.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 736 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   priceUSD  transactions     size  sentbyaddress  difficulty      hashrate  \\\n",
       "0    0.0495           235  649.653            390     181.543  2.775561e+09   \n",
       "1    0.0726           248  765.285            424     181.543  1.554461e+09   \n",
       "2    0.0859           354  756.040            553     181.543  1.551287e+09   \n",
       "3    0.0783           413  984.707            632     181.543  1.640430e+09   \n",
       "4    0.0767           256  542.483            440     181.543  1.723493e+09   \n",
       "\n",
       "   mining_profitability  sentinusdUSD  transactionfeesUSD  \\\n",
       "0              154298.0        1193.0            0.000010   \n",
       "1              401834.0        2620.0            0.000243   \n",
       "2              481473.0        4048.0            0.000022   \n",
       "3              431831.0        2341.0            0.000000   \n",
       "4              460783.0        2122.0            0.000000   \n",
       "\n",
       "   median_transaction_feeUSD  ...  price3rsiUSD  price7rsiUSD  price14rsiUSD  \\\n",
       "0                        0.0  ...         0.000           0.0            0.0   \n",
       "1                        0.0  ...         0.000           0.0            0.0   \n",
       "2                        0.0  ...         0.000           0.0            0.0   \n",
       "3                        0.0  ...        82.751           0.0            0.0   \n",
       "4                        0.0  ...        78.603           0.0            0.0   \n",
       "\n",
       "   price30rsiUSD  price90rsiUSD  price3rocUSD  price7rocUSD  price14rocUSD  \\\n",
       "0            0.0            0.0         0.000           0.0            0.0   \n",
       "1            0.0            0.0         0.000           0.0            0.0   \n",
       "2            0.0            0.0         0.000           0.0            0.0   \n",
       "3            0.0            0.0        58.099           0.0            0.0   \n",
       "4            0.0            0.0         5.652           0.0            0.0   \n",
       "\n",
       "   price30rocUSD  price90rocUSD  \n",
       "0            0.0            0.0  \n",
       "1            0.0            0.0  \n",
       "2            0.0            0.0  \n",
       "3            0.0            0.0  \n",
       "4            0.0            0.0  \n",
       "\n",
       "[5 rows x 736 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0689fd",
   "metadata": {},
   "source": [
    "### Dropping the priceUSD column and storing it in 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7d90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_13124\\646315669.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  n=df1.drop('priceUSD',1)\n"
     ]
    }
   ],
   "source": [
    "df1=df_new.reset_index(drop=True)\n",
    "n=df1.drop('priceUSD',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3770ecb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>9349.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>9394.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>9366.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>9393.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3487</th>\n",
       "      <td>9398.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3488 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       priceUSD\n",
       "0        0.0495\n",
       "1        0.0726\n",
       "2        0.0859\n",
       "3        0.0783\n",
       "4        0.0767\n",
       "...         ...\n",
       "3483  9349.0000\n",
       "3484  9394.0000\n",
       "3485  9366.0000\n",
       "3486  9393.0000\n",
       "3487  9398.0000\n",
       "\n",
       "[3488 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df1[['priceUSD']]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedd650",
   "metadata": {},
   "source": [
    "### Reading the MINMAX PCA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7cbe8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>priceUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.655104</td>\n",
       "      <td>0.802061</td>\n",
       "      <td>1.817268</td>\n",
       "      <td>-1.285273</td>\n",
       "      <td>-1.640494</td>\n",
       "      <td>1.700293</td>\n",
       "      <td>-1.001414</td>\n",
       "      <td>1.101779</td>\n",
       "      <td>-0.999867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153301</td>\n",
       "      <td>-0.027586</td>\n",
       "      <td>0.225252</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.170539</td>\n",
       "      <td>0.097393</td>\n",
       "      <td>0.146309</td>\n",
       "      <td>0.208756</td>\n",
       "      <td>1.173401</td>\n",
       "      <td>0.0495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.614658</td>\n",
       "      <td>0.803416</td>\n",
       "      <td>1.686412</td>\n",
       "      <td>-1.009045</td>\n",
       "      <td>-1.609771</td>\n",
       "      <td>1.699671</td>\n",
       "      <td>-0.747059</td>\n",
       "      <td>1.416030</td>\n",
       "      <td>-0.615753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261052</td>\n",
       "      <td>-0.121278</td>\n",
       "      <td>0.115798</td>\n",
       "      <td>0.108756</td>\n",
       "      <td>0.043790</td>\n",
       "      <td>0.147877</td>\n",
       "      <td>0.246103</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>1.104297</td>\n",
       "      <td>0.0726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-3.636358</td>\n",
       "      <td>0.825494</td>\n",
       "      <td>1.710698</td>\n",
       "      <td>-1.048296</td>\n",
       "      <td>-1.626373</td>\n",
       "      <td>1.841579</td>\n",
       "      <td>-0.698989</td>\n",
       "      <td>1.496502</td>\n",
       "      <td>-0.499075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356758</td>\n",
       "      <td>-0.111855</td>\n",
       "      <td>0.145736</td>\n",
       "      <td>-0.008782</td>\n",
       "      <td>0.276770</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>0.211571</td>\n",
       "      <td>0.214689</td>\n",
       "      <td>0.931091</td>\n",
       "      <td>0.0859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-3.641610</td>\n",
       "      <td>0.745754</td>\n",
       "      <td>1.603314</td>\n",
       "      <td>-1.195112</td>\n",
       "      <td>-0.952689</td>\n",
       "      <td>1.644297</td>\n",
       "      <td>-0.151961</td>\n",
       "      <td>1.282045</td>\n",
       "      <td>-0.588409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.076649</td>\n",
       "      <td>0.157377</td>\n",
       "      <td>-0.213748</td>\n",
       "      <td>0.256696</td>\n",
       "      <td>0.187187</td>\n",
       "      <td>0.140654</td>\n",
       "      <td>0.291811</td>\n",
       "      <td>0.537814</td>\n",
       "      <td>0.0783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-3.646880</td>\n",
       "      <td>0.760457</td>\n",
       "      <td>1.639556</td>\n",
       "      <td>-1.193579</td>\n",
       "      <td>-1.102717</td>\n",
       "      <td>1.673044</td>\n",
       "      <td>-0.434564</td>\n",
       "      <td>1.083872</td>\n",
       "      <td>-0.812450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285444</td>\n",
       "      <td>0.095544</td>\n",
       "      <td>0.122837</td>\n",
       "      <td>-0.236417</td>\n",
       "      <td>0.286961</td>\n",
       "      <td>0.041868</td>\n",
       "      <td>0.133825</td>\n",
       "      <td>0.244540</td>\n",
       "      <td>0.446112</td>\n",
       "      <td>0.0767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0 -3.655104  0.802061  1.817268 -1.285273 -1.640494  1.700293   \n",
       "1           1 -3.614658  0.803416  1.686412 -1.009045 -1.609771  1.699671   \n",
       "2           2 -3.636358  0.825494  1.710698 -1.048296 -1.626373  1.841579   \n",
       "3           3 -3.641610  0.745754  1.603314 -1.195112 -0.952689  1.644297   \n",
       "4           4 -3.646880  0.760457  1.639556 -1.193579 -1.102717  1.673044   \n",
       "\n",
       "          6         7         8  ...        29        30        31        32  \\\n",
       "0 -1.001414  1.101779 -0.999867  ...  0.153301 -0.027586  0.225252  0.007899   \n",
       "1 -0.747059  1.416030 -0.615753  ...  0.261052 -0.121278  0.115798  0.108756   \n",
       "2 -0.698989  1.496502 -0.499075  ...  0.356758 -0.111855  0.145736 -0.008782   \n",
       "3 -0.151961  1.282045 -0.588409  ...  0.360500  0.076649  0.157377 -0.213748   \n",
       "4 -0.434564  1.083872 -0.812450  ...  0.285444  0.095544  0.122837 -0.236417   \n",
       "\n",
       "         33        34        35        36        37  priceUSD  \n",
       "0  0.170539  0.097393  0.146309  0.208756  1.173401    0.0495  \n",
       "1  0.043790  0.147877  0.246103  0.141600  1.104297    0.0726  \n",
       "2  0.276770  0.029030  0.211571  0.214689  0.931091    0.0859  \n",
       "3  0.256696  0.187187  0.140654  0.291811  0.537814    0.0783  \n",
       "4  0.286961  0.041868  0.133825  0.244540  0.446112    0.0767  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmaxPCA=pd.read_csv('PCA_MinMaxScaler_data1.csv')\n",
    "#Adding the y column to this dataset\n",
    "combined_data=minmaxPCA.assign(priceUSD=y)\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02482116",
   "metadata": {},
   "source": [
    "### Dropping the first unnamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665416d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>priceUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.655104</td>\n",
       "      <td>0.802061</td>\n",
       "      <td>1.817268</td>\n",
       "      <td>-1.285273</td>\n",
       "      <td>-1.640494</td>\n",
       "      <td>1.700293</td>\n",
       "      <td>-1.001414</td>\n",
       "      <td>1.101779</td>\n",
       "      <td>-0.999867</td>\n",
       "      <td>0.152710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153301</td>\n",
       "      <td>-0.027586</td>\n",
       "      <td>0.225252</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.170539</td>\n",
       "      <td>0.097393</td>\n",
       "      <td>0.146309</td>\n",
       "      <td>0.208756</td>\n",
       "      <td>1.173401</td>\n",
       "      <td>0.0495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.614658</td>\n",
       "      <td>0.803416</td>\n",
       "      <td>1.686412</td>\n",
       "      <td>-1.009045</td>\n",
       "      <td>-1.609771</td>\n",
       "      <td>1.699671</td>\n",
       "      <td>-0.747059</td>\n",
       "      <td>1.416030</td>\n",
       "      <td>-0.615753</td>\n",
       "      <td>0.506121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261052</td>\n",
       "      <td>-0.121278</td>\n",
       "      <td>0.115798</td>\n",
       "      <td>0.108756</td>\n",
       "      <td>0.043790</td>\n",
       "      <td>0.147877</td>\n",
       "      <td>0.246103</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>1.104297</td>\n",
       "      <td>0.0726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.636358</td>\n",
       "      <td>0.825494</td>\n",
       "      <td>1.710698</td>\n",
       "      <td>-1.048296</td>\n",
       "      <td>-1.626373</td>\n",
       "      <td>1.841579</td>\n",
       "      <td>-0.698989</td>\n",
       "      <td>1.496502</td>\n",
       "      <td>-0.499075</td>\n",
       "      <td>0.612378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356758</td>\n",
       "      <td>-0.111855</td>\n",
       "      <td>0.145736</td>\n",
       "      <td>-0.008782</td>\n",
       "      <td>0.276770</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>0.211571</td>\n",
       "      <td>0.214689</td>\n",
       "      <td>0.931091</td>\n",
       "      <td>0.0859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.641610</td>\n",
       "      <td>0.745754</td>\n",
       "      <td>1.603314</td>\n",
       "      <td>-1.195112</td>\n",
       "      <td>-0.952689</td>\n",
       "      <td>1.644297</td>\n",
       "      <td>-0.151961</td>\n",
       "      <td>1.282045</td>\n",
       "      <td>-0.588409</td>\n",
       "      <td>0.833923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.076649</td>\n",
       "      <td>0.157377</td>\n",
       "      <td>-0.213748</td>\n",
       "      <td>0.256696</td>\n",
       "      <td>0.187187</td>\n",
       "      <td>0.140654</td>\n",
       "      <td>0.291811</td>\n",
       "      <td>0.537814</td>\n",
       "      <td>0.0783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.646880</td>\n",
       "      <td>0.760457</td>\n",
       "      <td>1.639556</td>\n",
       "      <td>-1.193579</td>\n",
       "      <td>-1.102717</td>\n",
       "      <td>1.673044</td>\n",
       "      <td>-0.434564</td>\n",
       "      <td>1.083872</td>\n",
       "      <td>-0.812450</td>\n",
       "      <td>0.726314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285444</td>\n",
       "      <td>0.095544</td>\n",
       "      <td>0.122837</td>\n",
       "      <td>-0.236417</td>\n",
       "      <td>0.286961</td>\n",
       "      <td>0.041868</td>\n",
       "      <td>0.133825</td>\n",
       "      <td>0.244540</td>\n",
       "      <td>0.446112</td>\n",
       "      <td>0.0767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>4.827080</td>\n",
       "      <td>3.872729</td>\n",
       "      <td>-0.564335</td>\n",
       "      <td>-0.964944</td>\n",
       "      <td>0.955012</td>\n",
       "      <td>-0.208703</td>\n",
       "      <td>-0.160729</td>\n",
       "      <td>0.079293</td>\n",
       "      <td>-0.353848</td>\n",
       "      <td>0.195394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>0.315107</td>\n",
       "      <td>-0.007492</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.265255</td>\n",
       "      <td>0.171378</td>\n",
       "      <td>-0.052400</td>\n",
       "      <td>0.188256</td>\n",
       "      <td>0.122504</td>\n",
       "      <td>9349.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>4.874243</td>\n",
       "      <td>4.078286</td>\n",
       "      <td>-0.498325</td>\n",
       "      <td>-1.028046</td>\n",
       "      <td>0.857928</td>\n",
       "      <td>-0.213927</td>\n",
       "      <td>-0.295887</td>\n",
       "      <td>0.083715</td>\n",
       "      <td>-0.351354</td>\n",
       "      <td>0.203269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087225</td>\n",
       "      <td>0.475346</td>\n",
       "      <td>-0.080046</td>\n",
       "      <td>-0.166548</td>\n",
       "      <td>0.307902</td>\n",
       "      <td>0.089965</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.139277</td>\n",
       "      <td>0.202801</td>\n",
       "      <td>9394.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>4.873536</td>\n",
       "      <td>3.950721</td>\n",
       "      <td>-0.594500</td>\n",
       "      <td>-0.959722</td>\n",
       "      <td>1.011089</td>\n",
       "      <td>-0.306760</td>\n",
       "      <td>-0.063507</td>\n",
       "      <td>0.463026</td>\n",
       "      <td>-0.061280</td>\n",
       "      <td>0.456087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133529</td>\n",
       "      <td>0.307913</td>\n",
       "      <td>-0.229377</td>\n",
       "      <td>0.031294</td>\n",
       "      <td>0.255388</td>\n",
       "      <td>-0.049842</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.269696</td>\n",
       "      <td>0.125988</td>\n",
       "      <td>9366.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>4.822283</td>\n",
       "      <td>4.116169</td>\n",
       "      <td>-0.434845</td>\n",
       "      <td>-0.844725</td>\n",
       "      <td>0.491708</td>\n",
       "      <td>-0.136111</td>\n",
       "      <td>-0.857335</td>\n",
       "      <td>0.091674</td>\n",
       "      <td>-0.184611</td>\n",
       "      <td>0.583483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138564</td>\n",
       "      <td>0.315845</td>\n",
       "      <td>-0.106430</td>\n",
       "      <td>0.072532</td>\n",
       "      <td>0.355768</td>\n",
       "      <td>0.043088</td>\n",
       "      <td>-0.011649</td>\n",
       "      <td>0.177111</td>\n",
       "      <td>0.143189</td>\n",
       "      <td>9393.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3487</th>\n",
       "      <td>4.781473</td>\n",
       "      <td>4.086459</td>\n",
       "      <td>-0.413840</td>\n",
       "      <td>-0.719069</td>\n",
       "      <td>0.451225</td>\n",
       "      <td>-0.145129</td>\n",
       "      <td>-0.835111</td>\n",
       "      <td>0.209077</td>\n",
       "      <td>0.051204</td>\n",
       "      <td>0.802018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252241</td>\n",
       "      <td>0.203734</td>\n",
       "      <td>-0.005977</td>\n",
       "      <td>0.298842</td>\n",
       "      <td>0.464687</td>\n",
       "      <td>0.020750</td>\n",
       "      <td>-0.004168</td>\n",
       "      <td>0.162576</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>9398.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3488 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -3.655104  0.802061  1.817268 -1.285273 -1.640494  1.700293 -1.001414   \n",
       "1    -3.614658  0.803416  1.686412 -1.009045 -1.609771  1.699671 -0.747059   \n",
       "2    -3.636358  0.825494  1.710698 -1.048296 -1.626373  1.841579 -0.698989   \n",
       "3    -3.641610  0.745754  1.603314 -1.195112 -0.952689  1.644297 -0.151961   \n",
       "4    -3.646880  0.760457  1.639556 -1.193579 -1.102717  1.673044 -0.434564   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3483  4.827080  3.872729 -0.564335 -0.964944  0.955012 -0.208703 -0.160729   \n",
       "3484  4.874243  4.078286 -0.498325 -1.028046  0.857928 -0.213927 -0.295887   \n",
       "3485  4.873536  3.950721 -0.594500 -0.959722  1.011089 -0.306760 -0.063507   \n",
       "3486  4.822283  4.116169 -0.434845 -0.844725  0.491708 -0.136111 -0.857335   \n",
       "3487  4.781473  4.086459 -0.413840 -0.719069  0.451225 -0.145129 -0.835111   \n",
       "\n",
       "             7         8         9  ...        29        30        31  \\\n",
       "0     1.101779 -0.999867  0.152710  ...  0.153301 -0.027586  0.225252   \n",
       "1     1.416030 -0.615753  0.506121  ...  0.261052 -0.121278  0.115798   \n",
       "2     1.496502 -0.499075  0.612378  ...  0.356758 -0.111855  0.145736   \n",
       "3     1.282045 -0.588409  0.833923  ...  0.360500  0.076649  0.157377   \n",
       "4     1.083872 -0.812450  0.726314  ...  0.285444  0.095544  0.122837   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3483  0.079293 -0.353848  0.195394  ...  0.004526  0.315107 -0.007492   \n",
       "3484  0.083715 -0.351354  0.203269  ... -0.087225  0.475346 -0.080046   \n",
       "3485  0.463026 -0.061280  0.456087  ... -0.133529  0.307913 -0.229377   \n",
       "3486  0.091674 -0.184611  0.583483  ... -0.138564  0.315845 -0.106430   \n",
       "3487  0.209077  0.051204  0.802018  ... -0.252241  0.203734 -0.005977   \n",
       "\n",
       "            32        33        34        35        36        37   priceUSD  \n",
       "0     0.007899  0.170539  0.097393  0.146309  0.208756  1.173401     0.0495  \n",
       "1     0.108756  0.043790  0.147877  0.246103  0.141600  1.104297     0.0726  \n",
       "2    -0.008782  0.276770  0.029030  0.211571  0.214689  0.931091     0.0859  \n",
       "3    -0.213748  0.256696  0.187187  0.140654  0.291811  0.537814     0.0783  \n",
       "4    -0.236417  0.286961  0.041868  0.133825  0.244540  0.446112     0.0767  \n",
       "...        ...       ...       ...       ...       ...       ...        ...  \n",
       "3483  0.132500  0.265255  0.171378 -0.052400  0.188256  0.122504  9349.0000  \n",
       "3484 -0.166548  0.307902  0.089965  0.003772  0.139277  0.202801  9394.0000  \n",
       "3485  0.031294  0.255388 -0.049842  0.003653  0.269696  0.125988  9366.0000  \n",
       "3486  0.072532  0.355768  0.043088 -0.011649  0.177111  0.143189  9393.0000  \n",
       "3487  0.298842  0.464687  0.020750 -0.004168  0.162576 -0.000135  9398.0000  \n",
       "\n",
       "[3488 rows x 39 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldata = combined_data.iloc[: , 1:]\n",
    "finaldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd3216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = finaldata.iloc[:, :-1].values\n",
    "y = finaldata.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0e16d",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9f15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d0a92",
   "metadata": {},
   "source": [
    "### Training the Linear Regression model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f426bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eca17c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a0fa2",
   "metadata": {},
   "source": [
    "### Accuracy on linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76278b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score of trained model: 99.11862649440762\n",
      "Test score of trained model: 99.13277109132441\n"
     ]
    }
   ],
   "source": [
    "train_score = regressor.score(X_train, y_train)\n",
    "print(f'Train score of trained model: {train_score*100}')\n",
    "\n",
    "test_score = regressor.score(X_test, y_test)\n",
    "print(f'Test score of trained model: {test_score*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e2a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "k = X_test.shape[1]\n",
    "n = len(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "MAPE = np.mean(np.abs( (y_test-y_pred) / y_test))*100\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adj_r2 = 1-(1-r2) * (n-1)/(n-k-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec723809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>124687.983001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RMSE</td>\n",
       "      <td>353.111856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAE</td>\n",
       "      <td>235.832665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAPE</td>\n",
       "      <td>7143.543201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2</td>\n",
       "      <td>0.991328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adj_r2</td>\n",
       "      <td>0.990828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metric          Score\n",
       "0     MSE  124687.983001\n",
       "1    RMSE     353.111856\n",
       "2     MAE     235.832665\n",
       "3    MAPE    7143.543201\n",
       "4      r2       0.991328\n",
       "5  adj_r2       0.990828"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [MSE, RMSE, MAE, MAPE, r2, adj_r2]\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'MAPE', 'r2', 'adj_r2']\n",
    "\n",
    "table_results = pd.DataFrame({'Metric': metrics, 'Score': results})\n",
    "table_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c89bb3",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "473df538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score of trained model: 99.11852421018217\n",
      "Test score of trained model: 99.12919741651307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha = 0.001, normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "pred = ridge.predict(X_test)\n",
    "\n",
    "train_score = ridge.score(X_train, y_train)\n",
    "print(f'Train score of trained model: {train_score*100}')\n",
    "\n",
    "test_score = ridge.score(X_test, y_test)\n",
    "print(f'Test score of trained model: {test_score*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70fdef49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>125201.796943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RMSE</td>\n",
       "      <td>353.838659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAE</td>\n",
       "      <td>236.130506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2</td>\n",
       "      <td>0.991292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adj_r2</td>\n",
       "      <td>0.990790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metric          Score\n",
       "0     MSE  125201.796943\n",
       "1    RMSE     353.838659\n",
       "2     MAE     236.130506\n",
       "3      r2       0.991292\n",
       "4  adj_r2       0.990790"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = X_test.shape[1]\n",
    "n = len(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, pred)\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, pred))\n",
    "MAE = mean_absolute_error(y_test, pred)\n",
    "r2 = r2_score(y_test, pred)\n",
    "adj_r2 = 1-(1-r2) * (n-1)/(n-k-1)\n",
    "\n",
    "results = [MSE, RMSE, MAE, r2, adj_r2]\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'r2', 'adj_r2']\n",
    "\n",
    "table_results = pd.DataFrame({'Metric': metrics, 'Score': results})\n",
    "table_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427027d0",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd13aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score of trained model: 99.11862552425018\n",
      "Test score of trained model: 99.13259069160067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha = 0.001, normalize=True)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "pred = lasso.predict(X_test)\n",
    "\n",
    "train_score = lasso.score(X_train, y_train)\n",
    "print(f'Train score of trained model: {train_score*100}')\n",
    "\n",
    "test_score = lasso.score(X_test, y_test)\n",
    "print(f'Test score of trained model: {test_score*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35199aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>124713.920418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RMSE</td>\n",
       "      <td>353.148581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAE</td>\n",
       "      <td>235.821336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2</td>\n",
       "      <td>0.991326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adj_r2</td>\n",
       "      <td>0.990826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metric          Score\n",
       "0     MSE  124713.920418\n",
       "1    RMSE     353.148581\n",
       "2     MAE     235.821336\n",
       "3      r2       0.991326\n",
       "4  adj_r2       0.990826"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = X_test.shape[1]\n",
    "n = len(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, pred)\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, pred))\n",
    "MAE = mean_absolute_error(y_test, pred)\n",
    "r2 = r2_score(y_test, pred)\n",
    "adj_r2 = 1-(1-r2) * (n-1)/(n-k-1)\n",
    "\n",
    "results = [MSE, RMSE, MAE, r2, adj_r2]\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'r2', 'adj_r2']\n",
    "\n",
    "table_results = pd.DataFrame({'Metric': metrics, 'Score': results})\n",
    "table_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03039dcd",
   "metadata": {},
   "source": [
    "### Training the Kernel SVM model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36be2d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1767 2058  985 ... 1049 2116 2661]\n",
      "continuous\n",
      "multiclass\n",
      "multiclass\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(y_train)\n",
    "print(training_scores_encoded)\n",
    "print(utils.multiclass.type_of_target(y_train))\n",
    "print(utils.multiclass.type_of_target(y_train.astype('int')))\n",
    "print(utils.multiclass.type_of_target(training_scores_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e473ad7",
   "metadata": {},
   "source": [
    "### Applying grid search to find best model and the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a22e9c",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear']},\n",
    "              {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71a625fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf4fc7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 240 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n240 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 182, in fit\n    y = self._validate_targets(y)\n  File \"C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 735, in _validate_targets\n    check_classification_targets(y)\n  File \"C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\multiclass.py\", line 200, in check_classification_targets\n    raise ValueError(\"Unknown label type: %r\" % y_type)\nValueError: Unknown label type: 'continuous'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(SVC(),param_grid,refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\model_selection\\_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    850\u001b[0m     )\n\u001b[1;32m--> 852\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 240 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n240 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 182, in fit\n    y = self._validate_targets(y)\n  File \"C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 735, in _validate_targets\n    check_classification_targets(y)\n  File \"C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\multiclass.py\", line 200, in check_classification_targets\n    raise ValueError(\"Unknown label type: %r\" % y_type)\nValueError: Unknown label type: 'continuous'\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c96be4",
   "metadata": {},
   "source": [
    "### Gradient boosting Model and its parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4243e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GBR = GradientBoostingRegressor()\n",
    "parameters = {'learning_rate': [0.01,0.02,0.03,0.04],\n",
    "                  'subsample'    : [0.9, 0.5, 0.2, 0.1],\n",
    "                  'n_estimators' : [100,500,1000, 1500],\n",
    "                  'max_depth'    : [4,6,8,10]\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae3401b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.02, 0.03, 0.04],\n",
       "                         &#x27;max_depth&#x27;: [4, 6, 8, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 500, 1000, 1500],\n",
       "                         &#x27;subsample&#x27;: [0.9, 0.5, 0.2, 0.1]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.02, 0.03, 0.04],\n",
       "                         &#x27;max_depth&#x27;: [4, 6, 8, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 500, 1000, 1500],\n",
       "                         &#x27;subsample&#x27;: [0.9, 0.5, 0.2, 0.1]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.02, 0.03, 0.04],\n",
       "                         'max_depth': [4, 6, 8, 10],\n",
       "                         'n_estimators': [100, 500, 1000, 1500],\n",
       "                         'subsample': [0.9, 0.5, 0.2, 0.1]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters, cv = 2, n_jobs=-1)\n",
    "grid_GBR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d7a6be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score of trained model: 99.97503506172866\n",
      "Test score of trained model: 99.5896063640935\n"
     ]
    }
   ],
   "source": [
    "pred=grid_GBR.predict(X_test)\n",
    "train_score = grid_GBR.score(X_train, y_train)\n",
    "print(f'Train score of trained model: {train_score*100}')\n",
    "\n",
    "test_score = grid_GBR.score(X_test, y_test)\n",
    "print(f'Test score of trained model: {test_score*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d0288c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>59005.360852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RMSE</td>\n",
       "      <td>242.910191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAE</td>\n",
       "      <td>128.741309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2</td>\n",
       "      <td>0.995896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adj_r2</td>\n",
       "      <td>0.995659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metric         Score\n",
       "0     MSE  59005.360852\n",
       "1    RMSE    242.910191\n",
       "2     MAE    128.741309\n",
       "3      r2      0.995896\n",
       "4  adj_r2      0.995659"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = X_test.shape[1]\n",
    "n = len(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, pred)\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, pred))\n",
    "MAE = mean_absolute_error(y_test, pred)\n",
    "r2 = r2_score(y_test, pred)\n",
    "adj_r2 = 1-(1-r2) * (n-1)/(n-k-1)\n",
    "\n",
    "results = [MSE, RMSE, MAE, r2, adj_r2]\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'r2', 'adj_r2']\n",
    "\n",
    "table_results = pd.DataFrame({'Metric': metrics, 'Score': results})\n",
    "table_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d08ab6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " GradientBoostingRegressor(learning_rate=0.03, max_depth=4, n_estimators=1500,\n",
      "                          subsample=0.2)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9920817418549985\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.03, 'max_depth': 4, 'n_estimators': 1500, 'subsample': 0.2}\n"
     ]
    }
   ],
   "source": [
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3c24a",
   "metadata": {},
   "source": [
    "### Training the Random forest regression model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55fb4edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=10, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5001",
   "metadata": {},
   "source": [
    "### Randomforest regressor using GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "815f41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def Grid_Search_CV_RFR(X_train, y_train):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import ShuffleSplit\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "\n",
    "    estimator = RandomForestRegressor()\n",
    "    param_grid = { \n",
    "            \"n_estimators\"      : [10,20,30],\n",
    "            \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n",
    "            \"min_samples_split\" : [2,4,8],\n",
    "            \"bootstrap\": [True, False],\n",
    "            }\n",
    "\n",
    "    grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    return grid.best_score_ , grid.best_params_\n",
    "\n",
    "def RFR(X_train, X_test, y_train, y_test, best_params):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    estimator = RandomForestRegressor(n_jobs=-1).set_params(**best_params)\n",
    "    estimator.fit(X_train,y_train)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    print (\"R2 score:\",r2_score(y_test,y_predict))\n",
    "    k = X_test.shape[1]\n",
    "    n = len(X_test)\n",
    "\n",
    "    MSE = mean_squared_error(y_test, y_predict)\n",
    "    RMSE = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "    MAE = mean_absolute_error(y_test, y_predict)\n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "    adj_r2 = 1-(1-r2) * (n-1)/(n-k-1)\n",
    "\n",
    "    results = [MSE, RMSE, MAE, r2, adj_r2]\n",
    "    metrics = ['MSE', 'RMSE', 'MAE', 'r2', 'adj_r2']\n",
    "\n",
    "    table_results = pd.DataFrame({'Metric': metrics, 'Score': results})\n",
    "    print(table_results)\n",
    "    return y_test,y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c89d85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop:  0\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.9944382674939244\n",
      "   Metric         Score\n",
      "0     MSE  79965.185805\n",
      "1    RMSE    282.781162\n",
      "2     MAE    125.564509\n",
      "3      r2      0.994438\n",
      "4  adj_r2      0.994118\n",
      "Best Score: 0.9923497819830086\n",
      "Best params: {'bootstrap': True, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 30}\n",
      "Loop:  1\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.9933283736577053\n",
      "   Metric         Score\n",
      "0     MSE  95922.959168\n",
      "1    RMSE    309.714319\n",
      "2     MAE    128.824730\n",
      "3      r2      0.993328\n",
      "4  adj_r2      0.992944\n",
      "Best Score: 0.9925063032407749\n",
      "Best params: {'bootstrap': True, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 30}\n",
      "Loop:  2\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.9941210777043806\n",
      "   Metric         Score\n",
      "0     MSE  84525.660518\n",
      "1    RMSE    290.732971\n",
      "2     MAE    124.407251\n",
      "3      r2      0.994121\n",
      "4  adj_r2      0.993782\n",
      "Best Score: 0.992878882981237\n",
      "Best params: {'bootstrap': True, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 30}\n",
      "Loop:  3\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.9940079154768195\n",
      "   Metric         Score\n",
      "0     MSE  86152.678456\n",
      "1    RMSE    293.517765\n",
      "2     MAE    125.944895\n",
      "3      r2      0.994008\n",
      "4  adj_r2      0.993662\n",
      "Best Score: 0.9924286158465021\n",
      "Best params: {'bootstrap': True, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 20}\n",
      "Loop:  4\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.9936233577306293\n",
      "   Metric         Score\n",
      "0     MSE  91681.752642\n",
      "1    RMSE    302.789948\n",
      "2     MAE    129.808009\n",
      "3      r2      0.993623\n",
      "4  adj_r2      0.993256\n",
      "Best Score: 0.9924795405062243\n",
      "Best params: {'bootstrap': True, 'max_features': 'auto', 'min_samples_split': 4, 'n_estimators': 30}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print (\"Loop: \" , i)\n",
    "    print (\"--------------\")\n",
    "    best_score, best_params = Grid_Search_CV_RFR(X_train, y_train)\n",
    "    y_test , y_predict = RFR(X_train, X_test, y_train, y_test, best_params)\n",
    "    print (\"Best Score:\" ,best_score)\n",
    "    print (\"Best params:\",best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813cb6b8",
   "metadata": {},
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1f5e9",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f41996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU,ELU\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaf2669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=735,kernel_initializer='he_normal',activation='relu'))\n",
    "Dropout(0.2)\n",
    "classifier.add(Dense(units=32,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "Dropout(0.2)\n",
    "classifier.add(Dense(units=8,kernel_initializer='normal',activation='relu'))\n",
    "classifier.add(Dense(units=8,kernel_initializer='normal',activation='relu'))\n",
    "#Adding the output layer\n",
    "classifier.add(Dense(1,kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d43b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',loss='mean_absolute_error',metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42759983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2116.8113 - mean_absolute_error: 2113.4070 - val_loss: 2169.3162 - val_mean_absolute_error: 2160.4958\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1057.4983 - mean_absolute_error: 1033.9836 - val_loss: 406.0324 - val_mean_absolute_error: 376.6014\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 247.3941 - mean_absolute_error: 216.8650 - val_loss: 210.0865 - val_mean_absolute_error: 179.0924\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 176.2959 - mean_absolute_error: 145.5588 - val_loss: 182.5172 - val_mean_absolute_error: 152.0863\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 154.5115 - mean_absolute_error: 123.9923 - val_loss: 160.9641 - val_mean_absolute_error: 130.4518\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 139.9216 - mean_absolute_error: 109.4904 - val_loss: 157.1498 - val_mean_absolute_error: 126.5859\n",
      "Epoch 7/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 136.4698 - mean_absolute_error: 106.0749 - val_loss: 148.6258 - val_mean_absolute_error: 118.4248\n",
      "Epoch 8/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 129.2671 - mean_absolute_error: 99.0610 - val_loss: 145.9676 - val_mean_absolute_error: 116.0159\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 124.8320 - mean_absolute_error: 94.8360 - val_loss: 138.2331 - val_mean_absolute_error: 108.2966\n",
      "Epoch 10/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 119.8564 - mean_absolute_error: 89.9998 - val_loss: 135.9245 - val_mean_absolute_error: 106.1914\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 115.4240 - mean_absolute_error: 85.6740 - val_loss: 132.0690 - val_mean_absolute_error: 102.3013\n",
      "Epoch 12/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 110.5363 - mean_absolute_error: 80.9576 - val_loss: 126.6715 - val_mean_absolute_error: 97.1270\n",
      "Epoch 13/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 112.2219 - mean_absolute_error: 82.7983 - val_loss: 128.2005 - val_mean_absolute_error: 98.9143\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 106.6400 - mean_absolute_error: 77.3271 - val_loss: 118.9212 - val_mean_absolute_error: 89.5754\n",
      "Epoch 15/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 104.2599 - mean_absolute_error: 75.0775 - val_loss: 118.7727 - val_mean_absolute_error: 89.7130\n",
      "Epoch 16/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 103.5084 - mean_absolute_error: 74.5017 - val_loss: 114.6599 - val_mean_absolute_error: 85.7300\n",
      "Epoch 17/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 100.5867 - mean_absolute_error: 71.7835 - val_loss: 113.8050 - val_mean_absolute_error: 85.0974\n",
      "Epoch 18/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 99.2551 - mean_absolute_error: 70.5884 - val_loss: 108.8626 - val_mean_absolute_error: 80.3717\n",
      "Epoch 19/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 93.3175 - mean_absolute_error: 64.8715 - val_loss: 114.2789 - val_mean_absolute_error: 85.7754\n",
      "Epoch 20/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 90.9264 - mean_absolute_error: 62.6589 - val_loss: 104.6909 - val_mean_absolute_error: 76.4875\n",
      "Epoch 21/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 94.6307 - mean_absolute_error: 66.4614 - val_loss: 107.7885 - val_mean_absolute_error: 79.6625\n",
      "Epoch 22/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 90.3191 - mean_absolute_error: 62.3727 - val_loss: 101.1127 - val_mean_absolute_error: 73.2692\n",
      "Epoch 23/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 92.8567 - mean_absolute_error: 65.0472 - val_loss: 104.4305 - val_mean_absolute_error: 76.7761\n",
      "Epoch 24/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 88.8888 - mean_absolute_error: 61.3025 - val_loss: 98.9104 - val_mean_absolute_error: 71.4626\n",
      "Epoch 25/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 85.6487 - mean_absolute_error: 58.2263 - val_loss: 96.4132 - val_mean_absolute_error: 69.0546\n",
      "Epoch 26/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 85.4205 - mean_absolute_error: 58.1862 - val_loss: 95.7427 - val_mean_absolute_error: 68.5937\n",
      "Epoch 27/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 82.4322 - mean_absolute_error: 55.3494 - val_loss: 105.2109 - val_mean_absolute_error: 78.4450\n",
      "Epoch 28/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 84.5387 - mean_absolute_error: 57.6748 - val_loss: 98.1489 - val_mean_absolute_error: 71.4395\n",
      "Epoch 29/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 82.7190 - mean_absolute_error: 56.0460 - val_loss: 95.0582 - val_mean_absolute_error: 68.5323\n",
      "Epoch 30/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 79.6264 - mean_absolute_error: 53.1185 - val_loss: 90.1225 - val_mean_absolute_error: 63.6035\n",
      "Epoch 31/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 79.6610 - mean_absolute_error: 53.3255 - val_loss: 94.0374 - val_mean_absolute_error: 67.8557\n",
      "Epoch 32/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 76.2572 - mean_absolute_error: 50.0793 - val_loss: 87.5108 - val_mean_absolute_error: 61.4532\n",
      "Epoch 33/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 76.8197 - mean_absolute_error: 50.8976 - val_loss: 85.6664 - val_mean_absolute_error: 59.8170\n",
      "Epoch 34/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 75.6907 - mean_absolute_error: 49.9271 - val_loss: 91.6537 - val_mean_absolute_error: 65.9710\n",
      "Epoch 35/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 74.1722 - mean_absolute_error: 48.6077 - val_loss: 82.5066 - val_mean_absolute_error: 56.9812\n",
      "Epoch 36/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 74.3506 - mean_absolute_error: 48.9233 - val_loss: 86.6563 - val_mean_absolute_error: 61.3439\n",
      "Epoch 37/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 70.7048 - mean_absolute_error: 45.4668 - val_loss: 82.3531 - val_mean_absolute_error: 57.2196\n",
      "Epoch 38/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 71.9118 - mean_absolute_error: 46.8436 - val_loss: 85.6245 - val_mean_absolute_error: 60.5492\n",
      "Epoch 39/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 72.9640 - mean_absolute_error: 48.0338 - val_loss: 88.3095 - val_mean_absolute_error: 63.5756\n",
      "Epoch 40/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 70.3204 - mean_absolute_error: 45.6472 - val_loss: 82.9046 - val_mean_absolute_error: 58.3371\n",
      "Epoch 41/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 71.8379 - mean_absolute_error: 47.3161 - val_loss: 84.8246 - val_mean_absolute_error: 60.4280\n",
      "Epoch 42/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 69.6567 - mean_absolute_error: 45.2313 - val_loss: 81.4295 - val_mean_absolute_error: 57.0256\n",
      "Epoch 43/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 66.6813 - mean_absolute_error: 42.4079 - val_loss: 83.4839 - val_mean_absolute_error: 59.3379\n",
      "Epoch 44/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 68.6009 - mean_absolute_error: 44.4759 - val_loss: 85.5938 - val_mean_absolute_error: 61.6197\n",
      "Epoch 45/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 66.3683 - mean_absolute_error: 42.4501 - val_loss: 81.6087 - val_mean_absolute_error: 57.9049\n",
      "Epoch 46/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 67.5341 - mean_absolute_error: 43.8643 - val_loss: 94.5454 - val_mean_absolute_error: 71.1127\n",
      "Epoch 47/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 66.0232 - mean_absolute_error: 42.5105 - val_loss: 76.9340 - val_mean_absolute_error: 53.4286\n",
      "Epoch 48/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 69.1267 - mean_absolute_error: 45.7157 - val_loss: 84.4910 - val_mean_absolute_error: 61.1293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 66.5729 - mean_absolute_error: 43.3348 - val_loss: 75.2412 - val_mean_absolute_error: 52.0372\n",
      "Epoch 50/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 63.3829 - mean_absolute_error: 40.1896 - val_loss: 73.5875 - val_mean_absolute_error: 50.4605\n",
      "Epoch 51/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 64.3946 - mean_absolute_error: 41.3675 - val_loss: 71.2614 - val_mean_absolute_error: 48.2969\n",
      "Epoch 52/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 61.7172 - mean_absolute_error: 38.8613 - val_loss: 74.8366 - val_mean_absolute_error: 52.0432\n",
      "Epoch 53/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 62.7763 - mean_absolute_error: 40.0708 - val_loss: 98.7459 - val_mean_absolute_error: 75.9218\n",
      "Epoch 54/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 61.6144 - mean_absolute_error: 39.0391 - val_loss: 81.5255 - val_mean_absolute_error: 59.1005\n",
      "Epoch 55/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 62.4370 - mean_absolute_error: 39.9370 - val_loss: 71.9525 - val_mean_absolute_error: 49.5861\n",
      "Epoch 56/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 62.6966 - mean_absolute_error: 40.3588 - val_loss: 92.4437 - val_mean_absolute_error: 70.1243\n",
      "Epoch 57/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 62.3875 - mean_absolute_error: 40.2129 - val_loss: 74.7219 - val_mean_absolute_error: 52.6056\n",
      "Epoch 58/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 60.1869 - mean_absolute_error: 38.1277 - val_loss: 73.8107 - val_mean_absolute_error: 51.8610\n",
      "Epoch 59/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 60.1662 - mean_absolute_error: 38.2455 - val_loss: 90.7272 - val_mean_absolute_error: 69.0241\n",
      "Epoch 60/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 59.8246 - mean_absolute_error: 38.0184 - val_loss: 70.0649 - val_mean_absolute_error: 48.3231\n",
      "Epoch 61/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 59.6347 - mean_absolute_error: 37.9874 - val_loss: 72.2139 - val_mean_absolute_error: 50.5777\n",
      "Epoch 62/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 58.6498 - mean_absolute_error: 37.1167 - val_loss: 81.4057 - val_mean_absolute_error: 59.7935\n",
      "Epoch 63/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 57.8039 - mean_absolute_error: 36.3193 - val_loss: 74.0408 - val_mean_absolute_error: 52.6036\n",
      "Epoch 64/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 57.4698 - mean_absolute_error: 36.1789 - val_loss: 69.2374 - val_mean_absolute_error: 48.0552\n",
      "Epoch 65/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 61.8301 - mean_absolute_error: 40.6790 - val_loss: 72.5056 - val_mean_absolute_error: 51.3583\n",
      "Epoch 66/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 58.4929 - mean_absolute_error: 37.4610 - val_loss: 68.7342 - val_mean_absolute_error: 47.7266\n",
      "Epoch 67/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 56.2688 - mean_absolute_error: 35.3582 - val_loss: 74.9233 - val_mean_absolute_error: 54.1406\n",
      "Epoch 68/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 55.7504 - mean_absolute_error: 34.9958 - val_loss: 69.4858 - val_mean_absolute_error: 48.7915\n",
      "Epoch 69/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 55.2803 - mean_absolute_error: 34.5695 - val_loss: 69.9392 - val_mean_absolute_error: 49.2951\n",
      "Epoch 70/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 54.8228 - mean_absolute_error: 34.1758 - val_loss: 70.9535 - val_mean_absolute_error: 50.4487\n",
      "Epoch 71/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 57.0169 - mean_absolute_error: 36.4987 - val_loss: 66.9063 - val_mean_absolute_error: 46.4357\n",
      "Epoch 72/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 54.0259 - mean_absolute_error: 33.6172 - val_loss: 67.8673 - val_mean_absolute_error: 47.5728\n",
      "Epoch 73/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 55.1993 - mean_absolute_error: 34.9429 - val_loss: 68.4953 - val_mean_absolute_error: 48.2982\n",
      "Epoch 74/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 54.1819 - mean_absolute_error: 34.0358 - val_loss: 66.3404 - val_mean_absolute_error: 46.2398\n",
      "Epoch 75/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 52.2811 - mean_absolute_error: 32.2548 - val_loss: 64.3812 - val_mean_absolute_error: 44.3605\n",
      "Epoch 76/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 52.4700 - mean_absolute_error: 32.4995 - val_loss: 69.4590 - val_mean_absolute_error: 49.5399\n",
      "Epoch 77/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 54.3712 - mean_absolute_error: 34.4742 - val_loss: 64.8745 - val_mean_absolute_error: 45.0853\n",
      "Epoch 78/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 54.9428 - mean_absolute_error: 35.1928 - val_loss: 64.9589 - val_mean_absolute_error: 45.2922\n",
      "Epoch 79/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 54.7361 - mean_absolute_error: 35.0983 - val_loss: 83.1585 - val_mean_absolute_error: 63.7149\n",
      "Epoch 80/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 57.1119 - mean_absolute_error: 37.5762 - val_loss: 67.3941 - val_mean_absolute_error: 47.8523\n",
      "Epoch 81/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 55.0649 - mean_absolute_error: 35.6022 - val_loss: 66.8991 - val_mean_absolute_error: 47.4628\n",
      "Epoch 82/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 52.0601 - mean_absolute_error: 32.6476 - val_loss: 75.8509 - val_mean_absolute_error: 56.4365\n",
      "Epoch 83/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 53.3457 - mean_absolute_error: 34.0604 - val_loss: 66.8478 - val_mean_absolute_error: 47.5950\n",
      "Epoch 84/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.3024 - mean_absolute_error: 31.1371 - val_loss: 69.6356 - val_mean_absolute_error: 50.5610\n",
      "Epoch 85/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.4578 - mean_absolute_error: 31.3830 - val_loss: 64.1245 - val_mean_absolute_error: 45.1092\n",
      "Epoch 86/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 51.6703 - mean_absolute_error: 32.6948 - val_loss: 66.4627 - val_mean_absolute_error: 47.5526\n",
      "Epoch 87/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.7301 - mean_absolute_error: 31.8533 - val_loss: 65.0312 - val_mean_absolute_error: 46.1269\n",
      "Epoch 88/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 49.3242 - mean_absolute_error: 30.5138 - val_loss: 66.3280 - val_mean_absolute_error: 47.5760\n",
      "Epoch 89/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 51.6200 - mean_absolute_error: 32.8931 - val_loss: 62.4654 - val_mean_absolute_error: 43.8145\n",
      "Epoch 90/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 48.1659 - mean_absolute_error: 29.5676 - val_loss: 62.1788 - val_mean_absolute_error: 43.5655\n",
      "Epoch 91/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 49.7627 - mean_absolute_error: 31.2284 - val_loss: 68.1878 - val_mean_absolute_error: 49.6630\n",
      "Epoch 92/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.5582 - mean_absolute_error: 32.1189 - val_loss: 59.9446 - val_mean_absolute_error: 41.5649\n",
      "Epoch 93/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 49.6976 - mean_absolute_error: 31.3548 - val_loss: 66.5359 - val_mean_absolute_error: 48.2367\n",
      "Epoch 94/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 48.9501 - mean_absolute_error: 30.7117 - val_loss: 63.3427 - val_mean_absolute_error: 45.1921\n",
      "Epoch 95/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 52.4792 - mean_absolute_error: 34.3293 - val_loss: 63.6278 - val_mean_absolute_error: 45.4790\n",
      "Epoch 96/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 48.2842 - mean_absolute_error: 30.1952 - val_loss: 62.6084 - val_mean_absolute_error: 44.5118\n",
      "Epoch 97/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 47.2103 - mean_absolute_error: 29.1850 - val_loss: 67.4041 - val_mean_absolute_error: 49.5322\n",
      "Epoch 98/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 47.6213 - mean_absolute_error: 29.7212 - val_loss: 61.0008 - val_mean_absolute_error: 43.0815\n",
      "Epoch 99/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 49.9741 - mean_absolute_error: 32.1116 - val_loss: 62.9764 - val_mean_absolute_error: 45.1110\n",
      "Epoch 100/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 51.7100 - mean_absolute_error: 33.8957 - val_loss: 72.0888 - val_mean_absolute_error: 54.3689\n"
     ]
    }
   ],
   "source": [
    "model_history=classifier.fit(X_train,y_train,validation_split=0.2,batch_size=16,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1bf05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = classifier.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = classifier.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2fb5f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Mean Absolute Error Loss')\n",
    "plt.plot(model_history.history['loss'], label='train')\n",
    "plt.plot(model_history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fb29f",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d685facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=735,kernel_initializer='he_normal',activation='relu'))\n",
    "Dropout(0.2)\n",
    "classifier.add(Dense(units=32,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "Dropout(0.2)\n",
    "classifier.add(Dense(units=8,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "classifier.add(Dense(units=8,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "#Adding the output layer\n",
    "classifier.add(Dense(1,kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d5039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',loss='mean_absolute_error',metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2cc1ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2126.3418 - mean_absolute_error: 2124.0940 - val_loss: 2289.1755 - val_mean_absolute_error: 2289.0464\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1781.9093 - mean_absolute_error: 1774.3776 - val_loss: 974.8943 - val_mean_absolute_error: 954.6214\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 465.4309 - mean_absolute_error: 443.7422 - val_loss: 251.6695 - val_mean_absolute_error: 228.6064\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 189.9714 - mean_absolute_error: 166.6949 - val_loss: 175.1933 - val_mean_absolute_error: 151.7553\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 150.2961 - mean_absolute_error: 126.8401 - val_loss: 162.5774 - val_mean_absolute_error: 139.0245\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 136.3303 - mean_absolute_error: 112.8588 - val_loss: 155.0540 - val_mean_absolute_error: 131.3934\n",
      "Epoch 7/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 126.6717 - mean_absolute_error: 103.1059 - val_loss: 147.2156 - val_mean_absolute_error: 123.5564\n",
      "Epoch 8/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 121.4049 - mean_absolute_error: 97.9202 - val_loss: 141.0022 - val_mean_absolute_error: 117.6356\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 118.7505 - mean_absolute_error: 95.2973 - val_loss: 133.3251 - val_mean_absolute_error: 109.8447\n",
      "Epoch 10/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 110.3011 - mean_absolute_error: 86.7913 - val_loss: 128.4223 - val_mean_absolute_error: 104.9452\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 110.0012 - mean_absolute_error: 86.5796 - val_loss: 125.3839 - val_mean_absolute_error: 101.9189\n",
      "Epoch 12/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 105.3708 - mean_absolute_error: 81.9654 - val_loss: 130.0939 - val_mean_absolute_error: 106.6069\n",
      "Epoch 13/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 105.4620 - mean_absolute_error: 82.1446 - val_loss: 119.4762 - val_mean_absolute_error: 96.1577\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 98.8864 - mean_absolute_error: 75.5639 - val_loss: 121.1423 - val_mean_absolute_error: 97.9324\n",
      "Epoch 15/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 96.9721 - mean_absolute_error: 73.7199 - val_loss: 121.7159 - val_mean_absolute_error: 98.4458\n",
      "Epoch 16/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 97.0773 - mean_absolute_error: 73.8838 - val_loss: 115.9875 - val_mean_absolute_error: 92.8432\n",
      "Epoch 17/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 92.1403 - mean_absolute_error: 68.9786 - val_loss: 114.6519 - val_mean_absolute_error: 91.4252\n",
      "Epoch 18/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 92.3441 - mean_absolute_error: 69.2879 - val_loss: 108.5725 - val_mean_absolute_error: 85.6280\n",
      "Epoch 19/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 88.9679 - mean_absolute_error: 65.9725 - val_loss: 108.2139 - val_mean_absolute_error: 85.3536\n",
      "Epoch 20/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 87.9265 - mean_absolute_error: 64.9817 - val_loss: 104.4769 - val_mean_absolute_error: 81.6462\n",
      "Epoch 21/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 87.7214 - mean_absolute_error: 64.8523 - val_loss: 101.7804 - val_mean_absolute_error: 78.9899\n",
      "Epoch 22/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 86.2101 - mean_absolute_error: 63.3801 - val_loss: 107.4124 - val_mean_absolute_error: 84.7337\n",
      "Epoch 23/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 83.3390 - mean_absolute_error: 60.6252 - val_loss: 99.3323 - val_mean_absolute_error: 76.5946\n",
      "Epoch 24/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 83.5177 - mean_absolute_error: 60.8582 - val_loss: 95.1617 - val_mean_absolute_error: 72.4664\n",
      "Epoch 25/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 81.6670 - mean_absolute_error: 59.0432 - val_loss: 95.0800 - val_mean_absolute_error: 72.4741\n",
      "Epoch 26/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 78.0309 - mean_absolute_error: 55.4598 - val_loss: 92.2143 - val_mean_absolute_error: 69.6874\n",
      "Epoch 27/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 81.4820 - mean_absolute_error: 59.0235 - val_loss: 95.1107 - val_mean_absolute_error: 72.7373\n",
      "Epoch 28/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 77.6355 - mean_absolute_error: 55.2334 - val_loss: 90.1765 - val_mean_absolute_error: 67.8345\n",
      "Epoch 29/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 75.6661 - mean_absolute_error: 53.3657 - val_loss: 90.0184 - val_mean_absolute_error: 67.8229\n",
      "Epoch 30/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 76.2449 - mean_absolute_error: 54.0703 - val_loss: 89.2862 - val_mean_absolute_error: 67.1218\n",
      "Epoch 31/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 75.4191 - mean_absolute_error: 53.3620 - val_loss: 89.2762 - val_mean_absolute_error: 67.3209\n",
      "Epoch 32/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 74.2938 - mean_absolute_error: 52.3050 - val_loss: 83.0900 - val_mean_absolute_error: 61.0958\n",
      "Epoch 33/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 72.7464 - mean_absolute_error: 50.7920 - val_loss: 86.8635 - val_mean_absolute_error: 64.9676\n",
      "Epoch 34/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 71.1893 - mean_absolute_error: 49.3700 - val_loss: 87.3520 - val_mean_absolute_error: 65.6158\n",
      "Epoch 35/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 70.2241 - mean_absolute_error: 48.4807 - val_loss: 84.3155 - val_mean_absolute_error: 62.7365\n",
      "Epoch 36/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 74.9499 - mean_absolute_error: 53.3738 - val_loss: 83.4998 - val_mean_absolute_error: 62.0122\n",
      "Epoch 37/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 69.0657 - mean_absolute_error: 47.5659 - val_loss: 87.1493 - val_mean_absolute_error: 65.7268\n",
      "Epoch 38/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 71.3103 - mean_absolute_error: 49.9231 - val_loss: 79.6190 - val_mean_absolute_error: 58.2359\n",
      "Epoch 39/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 70.6786 - mean_absolute_error: 49.3792 - val_loss: 78.7729 - val_mean_absolute_error: 57.4939\n",
      "Epoch 40/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 65.5947 - mean_absolute_error: 44.3530 - val_loss: 77.0126 - val_mean_absolute_error: 55.8381\n",
      "Epoch 41/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 66.7204 - mean_absolute_error: 45.5369 - val_loss: 78.5714 - val_mean_absolute_error: 57.3948\n",
      "Epoch 42/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 69.7901 - mean_absolute_error: 48.6784 - val_loss: 78.9597 - val_mean_absolute_error: 57.9357\n",
      "Epoch 43/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 64.0573 - mean_absolute_error: 43.0568 - val_loss: 76.7681 - val_mean_absolute_error: 55.8628\n",
      "Epoch 44/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 66.1938 - mean_absolute_error: 45.2798 - val_loss: 78.0671 - val_mean_absolute_error: 57.2158\n",
      "Epoch 45/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 64.1250 - mean_absolute_error: 43.3142 - val_loss: 87.8621 - val_mean_absolute_error: 67.1350\n",
      "Epoch 46/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 65.6850 - mean_absolute_error: 44.9569 - val_loss: 75.4901 - val_mean_absolute_error: 54.7889\n",
      "Epoch 47/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 62.0464 - mean_absolute_error: 41.4164 - val_loss: 80.2839 - val_mean_absolute_error: 59.6668\n",
      "Epoch 48/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 64.1471 - mean_absolute_error: 43.5690 - val_loss: 78.0746 - val_mean_absolute_error: 57.5771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 61.3619 - mean_absolute_error: 40.8996 - val_loss: 74.2761 - val_mean_absolute_error: 53.8581\n",
      "Epoch 50/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 61.1455 - mean_absolute_error: 40.7498 - val_loss: 71.7102 - val_mean_absolute_error: 51.3523\n",
      "Epoch 51/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 61.4987 - mean_absolute_error: 41.1866 - val_loss: 75.5767 - val_mean_absolute_error: 55.3261\n",
      "Epoch 52/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 59.8671 - mean_absolute_error: 39.6767 - val_loss: 71.1318 - val_mean_absolute_error: 50.9874\n",
      "Epoch 53/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 59.1707 - mean_absolute_error: 39.0660 - val_loss: 70.8244 - val_mean_absolute_error: 50.7948\n",
      "Epoch 54/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 58.7544 - mean_absolute_error: 38.7665 - val_loss: 71.8060 - val_mean_absolute_error: 51.8685\n",
      "Epoch 55/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 59.8830 - mean_absolute_error: 40.0010 - val_loss: 100.9893 - val_mean_absolute_error: 81.0743\n",
      "Epoch 56/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 60.5622 - mean_absolute_error: 40.7939 - val_loss: 71.2377 - val_mean_absolute_error: 51.4713\n",
      "Epoch 57/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 56.4167 - mean_absolute_error: 36.7048 - val_loss: 75.0514 - val_mean_absolute_error: 55.3267\n",
      "Epoch 58/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 58.4419 - mean_absolute_error: 38.8244 - val_loss: 69.3853 - val_mean_absolute_error: 49.7554\n",
      "Epoch 59/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 57.0372 - mean_absolute_error: 37.4771 - val_loss: 70.0352 - val_mean_absolute_error: 50.4708\n",
      "Epoch 60/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 56.6247 - mean_absolute_error: 37.1287 - val_loss: 76.9714 - val_mean_absolute_error: 57.5567\n",
      "Epoch 61/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 59.2626 - mean_absolute_error: 39.8316 - val_loss: 77.8472 - val_mean_absolute_error: 58.5168\n",
      "Epoch 62/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 61.1333 - mean_absolute_error: 41.7721 - val_loss: 81.7182 - val_mean_absolute_error: 62.5042\n",
      "Epoch 63/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 55.9892 - mean_absolute_error: 36.7326 - val_loss: 66.9778 - val_mean_absolute_error: 47.7555\n",
      "Epoch 64/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 58.3111 - mean_absolute_error: 39.1591 - val_loss: 66.8895 - val_mean_absolute_error: 47.7948\n",
      "Epoch 65/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 58.2631 - mean_absolute_error: 39.1597 - val_loss: 70.7697 - val_mean_absolute_error: 51.6615\n",
      "Epoch 66/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 55.4799 - mean_absolute_error: 36.4362 - val_loss: 70.0231 - val_mean_absolute_error: 51.0584\n",
      "Epoch 67/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 55.0354 - mean_absolute_error: 36.1003 - val_loss: 67.2222 - val_mean_absolute_error: 48.3418\n",
      "Epoch 68/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 56.4445 - mean_absolute_error: 37.5733 - val_loss: 80.4667 - val_mean_absolute_error: 61.6985\n",
      "Epoch 69/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 55.5536 - mean_absolute_error: 36.7468 - val_loss: 74.1555 - val_mean_absolute_error: 55.4033\n",
      "Epoch 70/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 53.8038 - mean_absolute_error: 35.0693 - val_loss: 68.0654 - val_mean_absolute_error: 49.3900\n",
      "Epoch 71/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 53.9962 - mean_absolute_error: 35.3352 - val_loss: 65.4263 - val_mean_absolute_error: 46.7697\n",
      "Epoch 72/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 54.6381 - mean_absolute_error: 36.0467 - val_loss: 66.2294 - val_mean_absolute_error: 47.6895\n",
      "Epoch 73/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 54.9766 - mean_absolute_error: 36.5161 - val_loss: 64.9433 - val_mean_absolute_error: 46.5233\n",
      "Epoch 74/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 52.3041 - mean_absolute_error: 33.8576 - val_loss: 72.9699 - val_mean_absolute_error: 54.6232\n",
      "Epoch 75/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 52.9304 - mean_absolute_error: 34.5532 - val_loss: 66.0539 - val_mean_absolute_error: 47.6691\n",
      "Epoch 76/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.7346 - mean_absolute_error: 32.4192 - val_loss: 71.4518 - val_mean_absolute_error: 53.1457\n",
      "Epoch 77/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 51.6149 - mean_absolute_error: 33.3850 - val_loss: 65.1432 - val_mean_absolute_error: 46.9737\n",
      "Epoch 78/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.5178 - mean_absolute_error: 32.3652 - val_loss: 65.7653 - val_mean_absolute_error: 47.6913\n",
      "Epoch 79/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 51.8733 - mean_absolute_error: 33.8066 - val_loss: 69.9596 - val_mean_absolute_error: 51.9912\n",
      "Epoch 80/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 52.0208 - mean_absolute_error: 34.0491 - val_loss: 72.3685 - val_mean_absolute_error: 54.4232\n",
      "Epoch 81/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.1617 - mean_absolute_error: 32.2896 - val_loss: 60.7985 - val_mean_absolute_error: 42.9821\n",
      "Epoch 82/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.8746 - mean_absolute_error: 33.0564 - val_loss: 62.2680 - val_mean_absolute_error: 44.4773\n",
      "Epoch 83/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 51.1092 - mean_absolute_error: 33.3654 - val_loss: 71.8207 - val_mean_absolute_error: 54.1103\n",
      "Epoch 84/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 51.5259 - mean_absolute_error: 33.8059 - val_loss: 68.9306 - val_mean_absolute_error: 51.2092\n",
      "Epoch 85/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.3853 - mean_absolute_error: 32.7422 - val_loss: 63.0411 - val_mean_absolute_error: 45.3205\n",
      "Epoch 86/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 49.9766 - mean_absolute_error: 32.3768 - val_loss: 69.0262 - val_mean_absolute_error: 51.4382\n",
      "Epoch 87/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 50.0486 - mean_absolute_error: 32.5017 - val_loss: 60.0582 - val_mean_absolute_error: 42.5739\n",
      "Epoch 88/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 51.6413 - mean_absolute_error: 34.1800 - val_loss: 60.9609 - val_mean_absolute_error: 43.5490\n",
      "Epoch 89/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 49.6659 - mean_absolute_error: 32.2705 - val_loss: 59.8646 - val_mean_absolute_error: 42.4843\n",
      "Epoch 90/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 46.6054 - mean_absolute_error: 29.2676 - val_loss: 62.8915 - val_mean_absolute_error: 45.5885\n",
      "Epoch 91/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 46.8758 - mean_absolute_error: 29.5607 - val_loss: 59.5591 - val_mean_absolute_error: 42.2622\n",
      "Epoch 92/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 51.4645 - mean_absolute_error: 34.2020 - val_loss: 71.0912 - val_mean_absolute_error: 53.9218\n",
      "Epoch 93/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 50.8371 - mean_absolute_error: 33.6669 - val_loss: 60.1963 - val_mean_absolute_error: 43.0753\n",
      "Epoch 94/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 47.1026 - mean_absolute_error: 29.9939 - val_loss: 59.7734 - val_mean_absolute_error: 42.6884\n",
      "Epoch 95/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 50.4871 - mean_absolute_error: 33.4202 - val_loss: 63.6796 - val_mean_absolute_error: 46.6947\n",
      "Epoch 96/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 47.1574 - mean_absolute_error: 30.1650 - val_loss: 59.7561 - val_mean_absolute_error: 42.7747\n",
      "Epoch 97/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 48.6880 - mean_absolute_error: 31.7419 - val_loss: 58.2997 - val_mean_absolute_error: 41.3867\n",
      "Epoch 98/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 46.0747 - mean_absolute_error: 29.1942 - val_loss: 60.2744 - val_mean_absolute_error: 43.3916\n",
      "Epoch 99/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 48.8992 - mean_absolute_error: 32.0408 - val_loss: 60.6596 - val_mean_absolute_error: 43.8473\n",
      "Epoch 100/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 44.5793 - mean_absolute_error: 27.8034 - val_loss: 70.1766 - val_mean_absolute_error: 53.3574\n"
     ]
    }
   ],
   "source": [
    "model_history=classifier.fit(X_train,y_train,validation_split=0.2,batch_size=16,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535d579e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Mean Absolute Error Loss')\n",
    "plt.plot(model_history.history['loss'], label='train')\n",
    "plt.plot(model_history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdab76c",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ba1b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_squared_logarithmic_error\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=735,kernel_initializer='he_normal',activation='relu'))\n",
    "Dropout(0.2)\n",
    "classifier.add(Dense(units=32,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "Dropout(0.2)\n",
    "classifier.add(Dense(units=8,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "classifier.add(Dense(units=8,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "#Adding the output layer\n",
    "classifier.add(Dense(1,kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a6b8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',loss='mean_squared_logarithmic_error',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edca236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 30.3770 - mse: 15813155.0000 - val_loss: 14.5718 - val_mse: 17271544.0000\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 10.4196 - mse: 15246698.0000 - val_loss: 8.0159 - val_mse: 16498613.0000\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 6.5377 - mse: 14503308.0000 - val_loss: 5.6767 - val_mse: 15612922.0000\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 4.8606 - mse: 13648374.0000 - val_loss: 4.4041 - val_mse: 14668909.0000\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 3.8606 - mse: 12748597.0000 - val_loss: 3.5786 - val_mse: 13652786.0000\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 3.0339 - mse: 11498319.0000 - val_loss: 2.6703 - val_mse: 11958275.0000\n",
      "Epoch 7/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 2.3305 - mse: 9873509.0000 - val_loss: 2.1769 - val_mse: 10417202.0000\n",
      "Epoch 8/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.9335 - mse: 8562130.0000 - val_loss: 1.8374 - val_mse: 8918482.0000\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.6664 - mse: 7362980.0000 - val_loss: 1.5922 - val_mse: 7645876.0000\n",
      "Epoch 10/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.4767 - mse: 6350551.0000 - val_loss: 1.4277 - val_mse: 6628044.5000\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.3451 - mse: 5428834.0000 - val_loss: 1.3287 - val_mse: 5821398.0000\n",
      "Epoch 12/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2239 - mse: 4719723.0000 - val_loss: 1.1827 - val_mse: 4995906.5000\n",
      "Epoch 13/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.1277 - mse: 4083200.5000 - val_loss: 1.0902 - val_mse: 4320432.5000\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.0463 - mse: 3541355.5000 - val_loss: 1.0257 - val_mse: 3827668.7500\n",
      "Epoch 15/100\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.9772 - mse: 3142789.2500 - val_loss: 0.9755 - val_mse: 3418459.5000\n",
      "Epoch 16/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.9193 - mse: 2789751.7500 - val_loss: 0.8840 - val_mse: 3117251.7500\n",
      "Epoch 17/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.8532 - mse: 2559586.7500 - val_loss: 0.8308 - val_mse: 2775280.2500\n",
      "Epoch 18/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.8058 - mse: 2282466.7500 - val_loss: 0.7979 - val_mse: 2495494.0000\n",
      "Epoch 19/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.7530 - mse: 2032407.3750 - val_loss: 0.7292 - val_mse: 2262783.0000\n",
      "Epoch 20/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.7035 - mse: 1844422.5000 - val_loss: 0.6881 - val_mse: 2038158.8750\n",
      "Epoch 21/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.6739 - mse: 1710895.7500 - val_loss: 0.6428 - val_mse: 1853524.2500\n",
      "Epoch 22/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.6226 - mse: 1530329.0000 - val_loss: 0.6183 - val_mse: 1714347.8750\n",
      "Epoch 23/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5936 - mse: 1416657.5000 - val_loss: 0.5818 - val_mse: 1523189.3750\n",
      "Epoch 24/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5673 - mse: 1309829.2500 - val_loss: 0.5645 - val_mse: 1456093.7500\n",
      "Epoch 25/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5467 - mse: 1269395.6250 - val_loss: 0.5410 - val_mse: 1385732.6250\n",
      "Epoch 26/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5261 - mse: 1177666.7500 - val_loss: 0.5218 - val_mse: 1320710.2500\n",
      "Epoch 27/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5107 - mse: 1140497.2500 - val_loss: 0.5131 - val_mse: 1244429.5000\n",
      "Epoch 28/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4948 - mse: 1089506.3750 - val_loss: 0.4919 - val_mse: 1225870.6250\n",
      "Epoch 29/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4829 - mse: 1051127.3750 - val_loss: 0.4813 - val_mse: 1161446.0000\n",
      "Epoch 30/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4711 - mse: 989161.8750 - val_loss: 0.4723 - val_mse: 1120914.6250\n",
      "Epoch 31/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4633 - mse: 982163.0625 - val_loss: 0.4701 - val_mse: 1078348.3750\n",
      "Epoch 32/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4570 - mse: 940466.3750 - val_loss: 0.4490 - val_mse: 1053368.8750\n",
      "Epoch 33/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4361 - mse: 912115.0000 - val_loss: 0.4363 - val_mse: 1018531.3125\n",
      "Epoch 34/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4269 - mse: 870601.5000 - val_loss: 0.4257 - val_mse: 965623.7500\n",
      "Epoch 35/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4222 - mse: 855257.4375 - val_loss: 0.4161 - val_mse: 945405.5625\n",
      "Epoch 36/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4070 - mse: 824708.6250 - val_loss: 0.4057 - val_mse: 918287.9375\n",
      "Epoch 37/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3972 - mse: 814867.9375 - val_loss: 0.3962 - val_mse: 872677.3750\n",
      "Epoch 38/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3877 - mse: 773758.5625 - val_loss: 0.3877 - val_mse: 896400.6875\n",
      "Epoch 39/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3781 - mse: 768359.8125 - val_loss: 0.3778 - val_mse: 823222.1250\n",
      "Epoch 40/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3673 - mse: 749527.3125 - val_loss: 0.3664 - val_mse: 776323.5000\n",
      "Epoch 41/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3568 - mse: 709380.5000 - val_loss: 0.3554 - val_mse: 800459.5000\n",
      "Epoch 42/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3448 - mse: 686966.7500 - val_loss: 0.3431 - val_mse: 805824.8750\n",
      "Epoch 43/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3324 - mse: 693559.1875 - val_loss: 0.3330 - val_mse: 773793.6875\n",
      "Epoch 44/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3231 - mse: 675472.1250 - val_loss: 0.3212 - val_mse: 701766.1250\n",
      "Epoch 45/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3112 - mse: 640684.9375 - val_loss: 0.3119 - val_mse: 690172.5000\n",
      "Epoch 46/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3014 - mse: 624079.5000 - val_loss: 0.3014 - val_mse: 674949.9375\n",
      "Epoch 47/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2919 - mse: 589826.2500 - val_loss: 0.2941 - val_mse: 687588.6875\n",
      "Epoch 48/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2849 - mse: 586437.9375 - val_loss: 0.2860 - val_mse: 638793.0000\n",
      "Epoch 49/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2801 - mse: 562006.6875 - val_loss: 0.2823 - val_mse: 628270.5625\n",
      "Epoch 50/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2722 - mse: 556280.5000 - val_loss: 0.2741 - val_mse: 609442.3125\n",
      "Epoch 51/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2693 - mse: 526559.9375 - val_loss: 0.2746 - val_mse: 581893.1875\n",
      "Epoch 52/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2660 - mse: 519378.4062 - val_loss: 0.2673 - val_mse: 587340.5000\n",
      "Epoch 53/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2617 - mse: 502665.9062 - val_loss: 0.2631 - val_mse: 568629.1875\n",
      "Epoch 54/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2567 - mse: 503820.0000 - val_loss: 0.2590 - val_mse: 523048.6562\n",
      "Epoch 55/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2568 - mse: 477148.2188 - val_loss: 0.2567 - val_mse: 544019.7500\n",
      "Epoch 56/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2487 - mse: 459041.1875 - val_loss: 0.2510 - val_mse: 513113.5312\n",
      "Epoch 57/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2466 - mse: 454005.8125 - val_loss: 0.2451 - val_mse: 477444.0625\n",
      "Epoch 58/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2395 - mse: 432410.2188 - val_loss: 0.2406 - val_mse: 491812.4688\n",
      "Epoch 59/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2362 - mse: 432923.3125 - val_loss: 0.2402 - val_mse: 453628.7812\n",
      "Epoch 60/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 408732.9688 - val_loss: 0.2348 - val_mse: 434306.9062\n",
      "Epoch 61/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2319 - mse: 389854.3438 - val_loss: 0.2341 - val_mse: 460101.0000\n",
      "Epoch 62/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2306 - mse: 389118.0938 - val_loss: 0.2328 - val_mse: 436629.3750\n",
      "Epoch 63/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2288 - mse: 389402.3750 - val_loss: 0.2296 - val_mse: 417225.7188\n",
      "Epoch 64/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2257 - mse: 370308.5000 - val_loss: 0.2277 - val_mse: 419873.0938\n",
      "Epoch 65/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2248 - mse: 368105.4062 - val_loss: 0.2252 - val_mse: 399537.3750\n",
      "Epoch 66/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2239 - mse: 359716.3125 - val_loss: 0.2325 - val_mse: 416827.9375\n",
      "Epoch 67/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2233 - mse: 362249.7812 - val_loss: 0.2232 - val_mse: 383023.2188\n",
      "Epoch 68/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2186 - mse: 346797.7188 - val_loss: 0.2192 - val_mse: 378386.0625\n",
      "Epoch 69/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2166 - mse: 340615.1250 - val_loss: 0.2176 - val_mse: 366908.0312\n",
      "Epoch 70/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2148 - mse: 331159.4688 - val_loss: 0.2160 - val_mse: 353514.0312\n",
      "Epoch 71/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2143 - mse: 329277.1250 - val_loss: 0.2124 - val_mse: 363672.3438\n",
      "Epoch 72/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2104 - mse: 320929.7812 - val_loss: 0.2133 - val_mse: 349799.4688\n",
      "Epoch 73/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2082 - mse: 312006.2188 - val_loss: 0.2107 - val_mse: 337724.1875\n",
      "Epoch 74/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2048 - mse: 313774.2188 - val_loss: 0.2049 - val_mse: 341019.8438\n",
      "Epoch 75/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2024 - mse: 305521.2812 - val_loss: 0.2014 - val_mse: 311731.9688\n",
      "Epoch 76/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1995 - mse: 290451.6562 - val_loss: 0.1997 - val_mse: 339301.8438\n",
      "Epoch 77/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1949 - mse: 285540.5625 - val_loss: 0.1950 - val_mse: 311801.0625\n",
      "Epoch 78/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1916 - mse: 284466.4062 - val_loss: 0.1924 - val_mse: 305112.6875\n",
      "Epoch 79/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1875 - mse: 269093.0625 - val_loss: 0.1885 - val_mse: 298222.0938\n",
      "Epoch 80/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1875 - mse: 267470.4688 - val_loss: 0.1890 - val_mse: 306835.8750\n",
      "Epoch 81/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1848 - mse: 262282.0938 - val_loss: 0.1890 - val_mse: 276260.3125\n",
      "Epoch 82/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1847 - mse: 247910.8594 - val_loss: 0.1873 - val_mse: 302874.6562\n",
      "Epoch 83/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1837 - mse: 256364.9844 - val_loss: 0.1842 - val_mse: 288360.1562\n",
      "Epoch 84/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1815 - mse: 252912.4531 - val_loss: 0.1835 - val_mse: 279714.5625\n",
      "Epoch 85/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1811 - mse: 247789.1250 - val_loss: 0.1832 - val_mse: 255289.3438\n",
      "Epoch 86/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1809 - mse: 238337.2969 - val_loss: 0.1831 - val_mse: 279104.3438\n",
      "Epoch 87/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1800 - mse: 243479.5938 - val_loss: 0.1834 - val_mse: 254840.2031\n",
      "Epoch 88/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1777 - mse: 237057.3906 - val_loss: 0.1803 - val_mse: 259139.4688\n",
      "Epoch 89/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1783 - mse: 232910.2031 - val_loss: 0.1784 - val_mse: 263888.4688\n",
      "Epoch 90/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1755 - mse: 232331.6094 - val_loss: 0.1790 - val_mse: 260219.9062\n",
      "Epoch 91/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1750 - mse: 227002.5469 - val_loss: 0.1757 - val_mse: 245334.7344\n",
      "Epoch 92/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1734 - mse: 221188.0312 - val_loss: 0.1752 - val_mse: 261884.8438\n",
      "Epoch 93/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1726 - mse: 224762.5469 - val_loss: 0.1746 - val_mse: 264727.4375\n",
      "Epoch 94/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1726 - mse: 221942.8906 - val_loss: 0.1741 - val_mse: 237750.3906\n",
      "Epoch 95/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1725 - mse: 222305.3281 - val_loss: 0.1736 - val_mse: 236482.0625\n",
      "Epoch 96/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1700 - mse: 213424.4688 - val_loss: 0.1705 - val_mse: 242651.2188\n",
      "Epoch 97/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1708 - mse: 213022.5938 - val_loss: 0.1717 - val_mse: 263972.4062\n",
      "Epoch 98/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1687 - mse: 217452.2500 - val_loss: 0.1751 - val_mse: 239106.5781\n",
      "Epoch 99/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1702 - mse: 217948.7812 - val_loss: 0.1688 - val_mse: 235796.3906\n",
      "Epoch 100/100\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1699 - mse: 216861.2500 - val_loss: 0.1684 - val_mse: 238307.2656\n"
     ]
    }
   ],
   "source": [
    "history=classifier.fit(X_train,y_train,validation_split=0.2,batch_size=16,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c23f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss during training\n",
    "plt.title('Mean Squared Logarithmic Error Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c21fd",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7afdde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model for MSE Loss\n",
    "#mean_squared_error\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=735,kernel_initializer='he_normal',activation='relu'))\n",
    "Dropout(0.2)\n",
    "classifier.add(Dense(units=32,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "Dropout(0.2)\n",
    "classifier.add(Dense(units=8,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "classifier.add(Dense(units=8,kernel_initializer='normal',activation='relu',kernel_regularizer = 'l1'))\n",
    "#Adding the output layer\n",
    "classifier.add(Dense(1,kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea21544",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',loss='mean_squared_error',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94089675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "224/224 [==============================] - 1s 4ms/step - loss: 11483884.0000 - mse: 11483854.0000 - val_loss: 1240167.6250 - val_mse: 1240126.3750\n",
      "Epoch 2/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 434550.1875 - mse: 434508.0938 - val_loss: 195774.0469 - val_mse: 195730.8906\n",
      "Epoch 3/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 112932.8125 - mse: 112889.0000 - val_loss: 107873.6250 - val_mse: 107829.4531\n",
      "Epoch 4/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 75601.5469 - mse: 75557.0703 - val_loss: 85242.3828 - val_mse: 85197.6953\n",
      "Epoch 5/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 59867.7305 - mse: 59822.7656 - val_loss: 73004.3516 - val_mse: 72959.0000\n",
      "Epoch 6/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 53676.1836 - mse: 53630.4258 - val_loss: 61716.5820 - val_mse: 61670.4531\n",
      "Epoch 7/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 45433.2617 - mse: 45386.7695 - val_loss: 57669.8984 - val_mse: 57622.9844\n",
      "Epoch 8/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 43186.0195 - mse: 43138.8438 - val_loss: 74665.8047 - val_mse: 74618.0156\n",
      "Epoch 9/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 40406.0273 - mse: 40358.1836 - val_loss: 54808.8516 - val_mse: 54760.5508\n",
      "Epoch 10/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 37188.4219 - mse: 37139.9531 - val_loss: 42711.4375 - val_mse: 42662.7109\n",
      "Epoch 11/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 33974.9844 - mse: 33925.9766 - val_loss: 38260.1523 - val_mse: 38210.8477\n",
      "Epoch 12/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 26671.3770 - mse: 26621.8242 - val_loss: 37331.0703 - val_mse: 37281.3359\n",
      "Epoch 13/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 26639.5977 - mse: 26589.6191 - val_loss: 33506.0586 - val_mse: 33455.9180\n",
      "Epoch 14/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 27195.6211 - mse: 27145.2285 - val_loss: 31818.2930 - val_mse: 31767.5449\n",
      "Epoch 15/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 23001.5000 - mse: 22950.6445 - val_loss: 30931.4121 - val_mse: 30880.4590\n",
      "Epoch 16/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 20059.5547 - mse: 20008.3359 - val_loss: 27692.6094 - val_mse: 27641.0820\n",
      "Epoch 17/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 19299.7578 - mse: 19248.1270 - val_loss: 24263.7344 - val_mse: 24211.9121\n",
      "Epoch 18/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 17380.5898 - mse: 17328.6055 - val_loss: 36251.2773 - val_mse: 36199.3359\n",
      "Epoch 19/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 17266.4277 - mse: 17214.1660 - val_loss: 24429.6465 - val_mse: 24377.2773\n",
      "Epoch 20/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 17809.8926 - mse: 17757.2695 - val_loss: 23663.7988 - val_mse: 23610.9395\n",
      "Epoch 21/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 16390.0195 - mse: 16337.0967 - val_loss: 26356.9316 - val_mse: 26303.7031\n",
      "Epoch 22/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 14593.9248 - mse: 14540.6895 - val_loss: 25344.0137 - val_mse: 25290.7695\n",
      "Epoch 23/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 13820.1523 - mse: 13766.5840 - val_loss: 18130.9355 - val_mse: 18077.2891\n",
      "Epoch 24/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 12046.1738 - mse: 11992.3535 - val_loss: 16755.5684 - val_mse: 16701.5996\n",
      "Epoch 25/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 11628.6914 - mse: 11574.6318 - val_loss: 16866.1309 - val_mse: 16811.9668\n",
      "Epoch 26/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 11834.1143 - mse: 11779.8438 - val_loss: 17317.0156 - val_mse: 17262.7188\n",
      "Epoch 27/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 11496.1357 - mse: 11441.6445 - val_loss: 14453.9463 - val_mse: 14399.3945\n",
      "Epoch 28/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 11376.6738 - mse: 11321.9795 - val_loss: 13870.2461 - val_mse: 13815.4053\n",
      "Epoch 29/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 11057.7852 - mse: 11002.8447 - val_loss: 17723.7539 - val_mse: 17668.7148\n",
      "Epoch 30/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 9621.3516 - mse: 9566.2061 - val_loss: 14287.5127 - val_mse: 14232.3555\n",
      "Epoch 31/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 9668.4551 - mse: 9613.1465 - val_loss: 12813.3604 - val_mse: 12757.9883\n",
      "Epoch 32/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 9641.5811 - mse: 9586.1572 - val_loss: 13482.9912 - val_mse: 13427.4893\n",
      "Epoch 33/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 9451.8389 - mse: 9396.2285 - val_loss: 12657.6953 - val_mse: 12601.9834\n",
      "Epoch 34/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 9846.0156 - mse: 9790.2568 - val_loss: 14040.6133 - val_mse: 13984.7695\n",
      "Epoch 35/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 7888.0171 - mse: 7832.0879 - val_loss: 14077.2363 - val_mse: 14021.0566\n",
      "Epoch 36/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 8268.0127 - mse: 8211.8701 - val_loss: 11358.6719 - val_mse: 11302.5195\n",
      "Epoch 37/100\n",
      "224/224 [==============================] - 1s 3ms/step - loss: 8426.2920 - mse: 8370.1055 - val_loss: 12313.0918 - val_mse: 12256.8232\n",
      "Epoch 38/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 7765.6650 - mse: 7709.2939 - val_loss: 11676.8506 - val_mse: 11620.3701\n",
      "Epoch 39/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 7570.9980 - mse: 7514.4990 - val_loss: 10761.9658 - val_mse: 10705.3779\n",
      "Epoch 40/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 7151.3350 - mse: 7094.7017 - val_loss: 10964.7285 - val_mse: 10908.0059\n",
      "Epoch 41/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 7764.5942 - mse: 7707.8442 - val_loss: 11585.9854 - val_mse: 11529.1738\n",
      "Epoch 42/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 7217.6982 - mse: 7160.7764 - val_loss: 10492.0273 - val_mse: 10435.0635\n",
      "Epoch 43/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 6993.0513 - mse: 6936.0679 - val_loss: 16299.8369 - val_mse: 16242.8516\n",
      "Epoch 44/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 8921.5479 - mse: 8864.5010 - val_loss: 36048.4141 - val_mse: 35991.5273\n",
      "Epoch 45/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 10179.4346 - mse: 10122.2285 - val_loss: 14119.4072 - val_mse: 14062.0928\n",
      "Epoch 46/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 6125.3896 - mse: 6068.0854 - val_loss: 10121.4707 - val_mse: 10064.1416\n",
      "Epoch 47/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 5609.1431 - mse: 5551.7271 - val_loss: 11155.1494 - val_mse: 11097.6797\n",
      "Epoch 48/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 5388.6357 - mse: 5331.1675 - val_loss: 10524.8467 - val_mse: 10467.3535\n",
      "Epoch 49/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 5741.1885 - mse: 5683.5947 - val_loss: 10749.4941 - val_mse: 10691.8984\n",
      "Epoch 50/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 5601.5474 - mse: 5543.9126 - val_loss: 13316.5820 - val_mse: 13259.0342\n",
      "Epoch 51/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 6645.4224 - mse: 6587.7363 - val_loss: 9777.4678 - val_mse: 9719.6924\n",
      "Epoch 52/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4767.3267 - mse: 4709.5059 - val_loss: 9910.0088 - val_mse: 9852.1172\n",
      "Epoch 53/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 6842.1460 - mse: 6784.2397 - val_loss: 14531.9189 - val_mse: 14474.0840\n",
      "Epoch 54/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 6287.0317 - mse: 6228.9893 - val_loss: 9554.2500 - val_mse: 9496.1250\n",
      "Epoch 55/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 6350.0093 - mse: 6291.8833 - val_loss: 14916.0879 - val_mse: 14857.8154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 5745.4849 - mse: 5687.2852 - val_loss: 9395.0361 - val_mse: 9336.7900\n",
      "Epoch 57/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4254.8848 - mse: 4196.6318 - val_loss: 9975.4062 - val_mse: 9917.0439\n",
      "Epoch 58/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4980.3218 - mse: 4921.9604 - val_loss: 14341.3896 - val_mse: 14283.0547\n",
      "Epoch 59/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 5140.8618 - mse: 5082.4380 - val_loss: 12294.5449 - val_mse: 12236.0645\n",
      "Epoch 60/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 6719.1494 - mse: 6660.6938 - val_loss: 8205.2598 - val_mse: 8146.7437\n",
      "Epoch 61/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4768.2930 - mse: 4709.7280 - val_loss: 9385.7002 - val_mse: 9327.1768\n",
      "Epoch 62/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4205.6172 - mse: 4146.9761 - val_loss: 8392.7119 - val_mse: 8334.1035\n",
      "Epoch 63/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 4683.5566 - mse: 4624.8716 - val_loss: 11387.6602 - val_mse: 11328.8799\n",
      "Epoch 64/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 6052.6641 - mse: 5993.9121 - val_loss: 8457.8203 - val_mse: 8399.0986\n",
      "Epoch 65/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 3869.5569 - mse: 3810.7703 - val_loss: 17210.9121 - val_mse: 17151.9395\n",
      "Epoch 66/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 10998.9229 - mse: 10940.1826 - val_loss: 7349.7852 - val_mse: 7290.9756\n",
      "Epoch 67/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3804.5498 - mse: 3745.6023 - val_loss: 7032.9453 - val_mse: 6973.9312\n",
      "Epoch 68/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4142.9648 - mse: 4083.9387 - val_loss: 7761.1274 - val_mse: 7702.1226\n",
      "Epoch 69/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3812.7405 - mse: 3753.6045 - val_loss: 7867.0518 - val_mse: 7807.9067\n",
      "Epoch 70/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3306.4741 - mse: 3247.2476 - val_loss: 7080.4419 - val_mse: 7021.2104\n",
      "Epoch 71/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4034.1492 - mse: 3974.8889 - val_loss: 7334.2222 - val_mse: 7274.9355\n",
      "Epoch 72/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3668.3779 - mse: 3609.0618 - val_loss: 7536.9937 - val_mse: 7477.6421\n",
      "Epoch 73/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 6768.1060 - mse: 6708.7451 - val_loss: 19067.7168 - val_mse: 19008.4707\n",
      "Epoch 74/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 4870.6372 - mse: 4811.2305 - val_loss: 13080.1465 - val_mse: 13020.8037\n",
      "Epoch 75/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 3799.1587 - mse: 3739.7122 - val_loss: 6357.9019 - val_mse: 6298.3809\n",
      "Epoch 76/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3268.7478 - mse: 3209.1780 - val_loss: 7410.1929 - val_mse: 7350.6279\n",
      "Epoch 77/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3220.9163 - mse: 3161.3271 - val_loss: 8929.9961 - val_mse: 8870.2783\n",
      "Epoch 78/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 4193.3926 - mse: 4133.7393 - val_loss: 6986.1548 - val_mse: 6926.4595\n",
      "Epoch 79/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 3750.8809 - mse: 3691.1299 - val_loss: 9595.0234 - val_mse: 9535.3242\n",
      "Epoch 80/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 4951.1675 - mse: 4891.4160 - val_loss: 9084.2549 - val_mse: 9024.4131\n",
      "Epoch 81/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4317.0928 - mse: 4257.2261 - val_loss: 6540.9111 - val_mse: 6481.0508\n",
      "Epoch 82/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 3851.4673 - mse: 3791.5610 - val_loss: 7296.5674 - val_mse: 7236.6968\n",
      "Epoch 83/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3150.2246 - mse: 3090.2866 - val_loss: 9010.9707 - val_mse: 8951.0254\n",
      "Epoch 84/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 3955.2625 - mse: 3895.2415 - val_loss: 7982.8071 - val_mse: 7922.7573\n",
      "Epoch 85/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3669.7319 - mse: 3609.6719 - val_loss: 7405.8262 - val_mse: 7345.7104\n",
      "Epoch 86/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 3167.1482 - mse: 3107.0308 - val_loss: 16670.8359 - val_mse: 16610.9062\n",
      "Epoch 87/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4213.7588 - mse: 4153.6514 - val_loss: 12767.5195 - val_mse: 12707.2539\n",
      "Epoch 88/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 6370.1001 - mse: 6309.9214 - val_loss: 8073.1704 - val_mse: 8012.9863\n",
      "Epoch 89/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 2886.8777 - mse: 2826.6177 - val_loss: 6078.1367 - val_mse: 6017.8765\n",
      "Epoch 90/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 2690.5938 - mse: 2630.3257 - val_loss: 6065.3613 - val_mse: 6005.0361\n",
      "Epoch 91/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 2883.1333 - mse: 2822.8262 - val_loss: 7053.4585 - val_mse: 6993.1743\n",
      "Epoch 92/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3624.9170 - mse: 3564.5581 - val_loss: 9386.4492 - val_mse: 9326.0234\n",
      "Epoch 93/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3823.3203 - mse: 3762.9453 - val_loss: 5385.2739 - val_mse: 5324.8105\n",
      "Epoch 94/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 4202.6333 - mse: 4142.1572 - val_loss: 14496.1035 - val_mse: 14435.6250\n",
      "Epoch 95/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 4504.7202 - mse: 4444.1890 - val_loss: 7896.0786 - val_mse: 7835.5898\n",
      "Epoch 96/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 4008.2500 - mse: 3947.6997 - val_loss: 6144.4849 - val_mse: 6083.9829\n",
      "Epoch 97/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 3580.2424 - mse: 3519.6389 - val_loss: 11526.0908 - val_mse: 11465.5439\n",
      "Epoch 98/100\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 2380.2886 - mse: 2319.6479 - val_loss: 5619.0508 - val_mse: 5558.4224\n",
      "Epoch 99/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 3706.5042 - mse: 3645.7778 - val_loss: 7536.0054 - val_mse: 7475.2285\n",
      "Epoch 100/100\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 2974.4016 - mse: 2913.6350 - val_loss: 6006.2559 - val_mse: 5945.5337\n"
     ]
    }
   ],
   "source": [
    "history=classifier.fit(X_train,y_train,validation_split=0.2,batch_size=10,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6d77e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss during training\n",
    "plt.title('Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588828da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
